{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from model.eval_voc import parse_voc_annotation\n",
    "from model.eval_gen import BatchGenerator\n",
    "from model.eval_utils import normalize, _sigmoid,makedirs,correct_yolo_boxes,do_nms,decode_netout,preprocess_input\n",
    "from model.eval_utils import get_yolo_boxes,compute_overlap,compute_ap,_softmax\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from model.yolo3 import yolo_body, tiny_yolo_body\n",
    "from model.mobilenet import mobilenetv2_yolo_body\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid generator\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "    #   Create the validation generator\n",
    "    ###############################  \n",
    "model_labels = [ \"aeroplane\", \"bicycle\", \"bird\",\"boat\",\"bottle\", \"bus\",\"car\", \"cat\",\"chair\", \"cow\", \"diningtable\", \"dog\",\"horse\",\"motorbike\",\"person\", \"pottedplant\", \"sheep\",\"sofa\", \"train\",\"tvmonitor\"]\n",
    "valid_ints, labels = parse_voc_annotation(\n",
    "        \"VOCdevkit/1VOC/Annotations/\", \n",
    "        \"VOCdevkit/1VOC/JPEGImages/\", \n",
    "        \"voc1.pkl\",\n",
    "        model_labels\n",
    ")\n",
    "\n",
    "labels = labels.keys() if len(model_labels) == 0 else model_labels\n",
    "labels = sorted(labels)\n",
    "\n",
    "model_anchors = [ 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326 ]\n",
    "print(\"valid generator\")\n",
    "valid_generator = BatchGenerator(\n",
    "        instances           = valid_ints, \n",
    "        anchors             = model_anchors,   \n",
    "        labels              = labels,        \n",
    "        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
    "        max_box_per_image   = 0,\n",
    "        batch_size          = 2,\n",
    "        min_net_size        = 288,\n",
    "        max_net_size        = 448,   \n",
    "        shuffle             = True, \n",
    "        jitter              = 0.0, \n",
    "        norm                = normalize\n",
    "    )\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model = yolo_body(Input(shape=(None,None,3)), 3 , 20)\n",
    "infer_model.load_weights(\"model_data/trained_weights_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate(infer_model, valid_generator)\n",
    "model = infer_model \n",
    "generator = valid_generator \n",
    "iou_threshold=0.5\n",
    "obj_thresh=0.5\n",
    "nms_thresh=0.45\n",
    "net_h=416\n",
    "net_w=416\n",
    "save_path=None\n",
    "\n",
    "# gather all detections and annotations\n",
    "all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size()) ] # array of none\n",
    "all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size()) ]\n",
    "\n",
    "generator.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(16,)\n",
      "(16, 5)\n"
     ]
    }
   ],
   "source": [
    "   # gather all detections and annotations\n",
    "for i in range(generator.size()) :\n",
    "        print(i)\n",
    "        raw_image = [generator.load_image(i)]\n",
    "        #print(\"generator = \"+str(i) +\" from \" + str(generator.size()) )\n",
    "        # make the boxes and the labels\n",
    "        pred_boxes = get_yolo_boxes(model, raw_image, net_h, net_w, generator.get_anchors(), obj_thresh, nms_thresh)[0]\n",
    "        \n",
    "        #print(\"generator anchor = \"+str(generator.get_anchors()) )\n",
    "        score = np.array([box.get_score() for box in pred_boxes])\n",
    "        print( score.shape )\n",
    "        pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "        \n",
    "        if len(pred_boxes) > 0:\n",
    "            pred_boxes = np.array([[box.xmin, box.ymin, box.xmax, box.ymax, box.get_score()] for box in pred_boxes]) \n",
    "            print(pred_boxes.shape)\n",
    "        else:\n",
    "            pred_boxes = np.array([[]])  \n",
    "        \n",
    "        # sort the boxes and the labels according to scores\n",
    "        score_sort = np.argsort(-score)\n",
    "        pred_labels = pred_labels[score_sort]\n",
    "        pred_boxes  = pred_boxes[score_sort]\n",
    "        \n",
    "        # copy detections to all_detections\n",
    "        for label in range(generator.num_classes()):\n",
    "            #print(\"copy detections to all_detections  label \"+str(label) )\n",
    "            all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "\n",
    "        annotations = generator.load_annotation(i)\n",
    "        \n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            #print(\"copy detections to all_annotations  label \"+str(label) )\n",
    "            all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_detections )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection group[ 16.  -2. 236. 444.   0.]\n",
      "detection group[  9. -10. 243. 456.   0.]\n",
      "detection group[-19.   5. 226. 479.   0.]\n",
      "detection group[ 17.   0. 232. 483.   0.]\n",
      "detection group[ 27.  14. 276. 472.   0.]\n",
      "detection group[ -2.  61. 240. 490.   0.]\n",
      "detection group[ -2. 178. 196. 497.   0.]\n",
      "detection group[  2. 185. 243. 488.   0.]\n",
      "detection group[215. 195. 350. 485.   0.]\n",
      "detection group[  9. 208. 181. 506.   0.]\n",
      "detection group[244. 200. 365. 508.   0.]\n",
      "detection group[ 48. 256. 209. 516.   0.]\n",
      "no AP0\n",
      "detection group[ 27.         224.         223.         498.           0.99978298]\n",
      "[[0.80941815 0.         0.26647936]]\n",
      "[0]\n",
      "detection group[229.         202.         340.         503.           0.84637016]\n",
      "[[0.04654621 0.87156924 0.        ]]\n",
      "[1]\n",
      "Scores[0.99978298 0.84637016]\n",
      "a true_positive = [1. 1.]\n",
      "b true_positive = [1. 1.]\n",
      "c true_positive = [1. 2.]\n",
      "num anno = 3.0\n",
      "recall = [0.33333333 0.66666667]\n",
      "precision = [1. 1.]\n",
      "cancer = [1. 2.]\n",
      "average_precisions = {0: 0}\n",
      "no AP2\n",
      "no AP3\n",
      "no AP4\n",
      "no AP5\n",
      "no AP6\n",
      "no AP7\n",
      "no AP8\n",
      "no AP9\n",
      "no AP10\n",
      "no AP11\n",
      "no AP12\n",
      "no AP13\n",
      "detection group[ 13.          16.         238.         464.           0.99987304]\n",
      "[[0.34501165 0.9052774  0.03939367]]\n",
      "[1]\n",
      "detection group[222.           0.         335.         496.           0.98992682]\n",
      "[[0.         0.06161186 0.94320939]]\n",
      "[2]\n",
      "Scores[0.99987304 0.98992682]\n",
      "a true_positive = [1. 1.]\n",
      "b true_positive = [1. 1.]\n",
      "c true_positive = [1. 2.]\n",
      "num anno = 3.0\n",
      "recall = [0.33333333 0.66666667]\n",
      "precision = [1. 1.]\n",
      "cancer = [1. 2.]\n",
      "average_precisions = {0: 0, 1: 0.6666666666666666, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0}\n",
      "no AP15\n",
      "no AP16\n",
      "no AP17\n",
      "no AP18\n",
      "no AP19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0.6666666666666666,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 0,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 0.6666666666666666,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 17: 0,\n",
       " 18: 0,\n",
       " 19: 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute mAP by comparing all detections and all annotations\n",
    "average_precisions = {}\n",
    "for label in  range(generator.num_classes()) :\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        #print(\"get average precisions every label\"+str(label) )\n",
    "       \n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "            \n",
    "            #print(\"group all detection \"+str(generator) )\n",
    "\n",
    "            for d in detections:\n",
    "                print(\"detection group\"+ str(d) )\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    #print( \"true_positive = {}\".format(true_positives) )\n",
    "                    continue\n",
    "                \n",
    "                #print(annotations.shape[0])\n",
    "                \n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                print(overlaps)\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                print(assigned_annotation)\n",
    "                max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                    \n",
    "                   # print( \"true_positive = {}\".format(true_positives) )\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                   # print( \"true_positive = {}\".format(true_positives) )\n",
    "                    \n",
    "\n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            print(\"no AP\" + str(label) )\n",
    "            continue\n",
    "\n",
    "        # sort by score\n",
    "        print(\"Scores{}\".format(scores) )\n",
    "        print( \"a true_positive = {}\".format(true_positives) )\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "        print( \"b true_positive = {}\".format(true_positives) )\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "        print( \"c true_positive = {}\".format(true_positives) )\n",
    "\n",
    "        # compute recall and precision\n",
    "        recall    = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "        print( \"num anno = {}\".format(num_annotations) )\n",
    "        print( \"recall = {}\".format(recall) )\n",
    "        print( \"precision = {}\".format(precision) )\n",
    "        print( \"cancer = {}\".format(np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)) )\n",
    "        # compute average precision\n",
    "        average_precision  = compute_ap(recall, precision)  \n",
    "        print( \"average_precisions = {}\".format(average_precisions) )\n",
    "        average_precisions[label] = average_precision\n",
    "        \n",
    "average_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplane: 0.0000\n",
      "bicycle: 0.6667\n",
      "bird: 0.0000\n",
      "boat: 0.0000\n",
      "bottle: 0.0000\n",
      "bus: 0.0000\n",
      "car: 0.0000\n",
      "cat: 0.0000\n",
      "chair: 0.0000\n",
      "cow: 0.0000\n",
      "diningtable: 0.0000\n",
      "dog: 0.0000\n",
      "horse: 0.0000\n",
      "motorbike: 0.0000\n",
      "person: 0.6667\n",
      "pottedplant: 0.0000\n",
      "sheep: 0.0000\n",
      "sofa: 0.0000\n",
      "train: 0.0000\n",
      "tvmonitor: 0.0000\n",
      "mAP: 0.0667\n"
     ]
    }
   ],
   "source": [
    "# print the score\n",
    "for label, average_precision in average_precisions.items():\n",
    "    print(labels[label] + ': {:.4f}'.format(average_precision))\n",
    "print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
