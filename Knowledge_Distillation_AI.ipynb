{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_uBhi585gQt2",
    "outputId": "af160177-e730-4edd-ed7a-0fb74047d016"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Input, Embedding, LSTM, Dense, Lambda, GaussianNoise, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, merge\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers import MaxPooling2D, Dropout, Dense, Flatten, Activation, Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy as logloss\n",
    "from keras.metrics import categorical_accuracy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1dtl7u_g70f"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK8cBHCjhCIl"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# convert y_train and y_test to categorical binary values \n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TQoj-dFLhEhr",
    "outputId": "7c10f38e-1629-4ee2-e0ec-c856ceee98fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "z-Sz-WQ5zZ65",
    "outputId": "71d200a8-069c-4e9e-e97f-8b0fdc474e6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFtmUKSEhHbu"
   },
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3QC7acYHhMiR",
    "outputId": "49c20d10-19fe-4bd8-f97f-396323e01388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshape them to batch_size, width,height,#channels\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the values\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44cw18qKhT_V"
   },
   "source": [
    "# Define Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "f4RVzgD_hSvo",
    "outputId": "94d5f256-8d8c-43e2-fb11-72b32e310181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Teacher model\n",
    "\n",
    "input_shape = (28, 28, 1) # Input shape of each image\n",
    "\n",
    "# Hyperparameters\n",
    "nb_filters = 64 # number of convolutional filters to use\n",
    "pool_size = (2, 2) # size of pooling area for max pooling\n",
    "kernel_size = (3, 3) # convolution kernel size\n",
    "\n",
    "teacher = Sequential()\n",
    "teacher.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "teacher.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "teacher.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "teacher.add(Dropout(0.25)) # For reguralization\n",
    "\n",
    "teacher.add(Flatten())\n",
    "teacher.add(Dense(128, activation='relu'))\n",
    "teacher.add(Dropout(0.5)) # For reguralization\n",
    "\n",
    "teacher.add(Dense(nb_classes))\n",
    "teacher.add(Activation('softmax')) # Note that we add a normal softmax layer to begin with\n",
    "\n",
    "teacher.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(teacher.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RHkGp4tha8A"
   },
   "source": [
    "# Define Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "vR_gAsUqhZjP",
    "outputId": "7fe1afdf-7ba2-41a6-b2b1-2bbd21c8a1fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Student model that is stand-alone. We will evaluate its accuracy compared to a teacher trained student model\n",
    "\n",
    "student = Sequential()\n",
    "student.add(Flatten(input_shape=input_shape))\n",
    "student.add(Dense(32, activation='relu'))\n",
    "student.add(Dropout(0.2))\n",
    "student.add(Dense(nb_classes))\n",
    "student.add(Activation('softmax'))\n",
    "\n",
    "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "student.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gc2uy8MUhiXs"
   },
   "source": [
    "# Training the Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "toF5EnoAhnQm",
    "outputId": "f4a4a220-8f15-44af-f66e-fc764951729f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 143s 2ms/step - loss: 0.3780 - acc: 0.8824 - val_loss: 0.0771 - val_acc: 0.9763\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.1109 - acc: 0.9671 - val_loss: 0.0469 - val_acc: 0.9847\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.0781 - acc: 0.9766 - val_loss: 0.0396 - val_acc: 0.9871\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.0640 - acc: 0.9807 - val_loss: 0.0329 - val_acc: 0.9885\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 147s 2ms/step - loss: 0.0558 - acc: 0.9829 - val_loss: 0.0387 - val_acc: 0.9873\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 156s 3ms/step - loss: 0.0497 - acc: 0.9850 - val_loss: 0.0318 - val_acc: 0.9891\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 158s 3ms/step - loss: 0.0442 - acc: 0.9864 - val_loss: 0.0329 - val_acc: 0.9882\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 154s 3ms/step - loss: 0.0391 - acc: 0.9881 - val_loss: 0.0275 - val_acc: 0.9904\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 159s 3ms/step - loss: 0.0350 - acc: 0.9890 - val_loss: 0.0258 - val_acc: 0.9911\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 150s 2ms/step - loss: 0.0325 - acc: 0.9902 - val_loss: 0.0252 - val_acc: 0.9915\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.0310 - acc: 0.9903 - val_loss: 0.0267 - val_acc: 0.9915\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 160s 3ms/step - loss: 0.0283 - acc: 0.9909 - val_loss: 0.0260 - val_acc: 0.9910\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 0.0262 - acc: 0.9920 - val_loss: 0.0264 - val_acc: 0.9912\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 154s 3ms/step - loss: 0.0254 - acc: 0.9917 - val_loss: 0.0262 - val_acc: 0.9916\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.0244 - acc: 0.9923 - val_loss: 0.0236 - val_acc: 0.9922\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 154s 3ms/step - loss: 0.0224 - acc: 0.9930 - val_loss: 0.0236 - val_acc: 0.9924\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.0216 - acc: 0.9929 - val_loss: 0.0242 - val_acc: 0.9925\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.0192 - acc: 0.9940 - val_loss: 0.0255 - val_acc: 0.9917\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 170s 3ms/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0251 - val_acc: 0.9925\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 149s 2ms/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0256 - val_acc: 0.9912\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 148s 2ms/step - loss: 0.0186 - acc: 0.9939 - val_loss: 0.0263 - val_acc: 0.9914\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0176 - acc: 0.9942 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.0168 - acc: 0.9942 - val_loss: 0.0245 - val_acc: 0.9920\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0277 - val_acc: 0.9922\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 149s 2ms/step - loss: 0.0159 - acc: 0.9948 - val_loss: 0.0271 - val_acc: 0.9919\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 159s 3ms/step - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0268 - val_acc: 0.9917\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0272 - val_acc: 0.9923\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.0139 - acc: 0.9956 - val_loss: 0.0274 - val_acc: 0.9923\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0258 - val_acc: 0.9928\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0270 - val_acc: 0.9923\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0280 - val_acc: 0.9930\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0247 - val_acc: 0.9927\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 148s 2ms/step - loss: 0.0126 - acc: 0.9959 - val_loss: 0.0253 - val_acc: 0.9932\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 150s 3ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0246 - val_acc: 0.9924\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 149s 2ms/step - loss: 0.0128 - acc: 0.9960 - val_loss: 0.0275 - val_acc: 0.9917\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0302 - val_acc: 0.9929\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 158s 3ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0297 - val_acc: 0.9924\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 158s 3ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0265 - val_acc: 0.9935\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0267 - val_acc: 0.9924\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0307 - val_acc: 0.9921\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 0.0110 - acc: 0.9962 - val_loss: 0.0310 - val_acc: 0.9921\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 158s 3ms/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0277 - val_acc: 0.9923\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 167s 3ms/step - loss: 0.0100 - acc: 0.9964 - val_loss: 0.0245 - val_acc: 0.9924\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 154s 3ms/step - loss: 0.0104 - acc: 0.9966 - val_loss: 0.0273 - val_acc: 0.9919\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 155s 3ms/step - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0296 - val_acc: 0.9928\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 159s 3ms/step - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0272 - val_acc: 0.9932\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 167s 3ms/step - loss: 0.0091 - acc: 0.9968 - val_loss: 0.0282 - val_acc: 0.9930\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 158s 3ms/step - loss: 0.0096 - acc: 0.9966 - val_loss: 0.0270 - val_acc: 0.9928\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 159s 3ms/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0281 - val_acc: 0.9935\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 165s 3ms/step - loss: 0.0095 - acc: 0.9968 - val_loss: 0.0273 - val_acc: 0.9938\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0292 - val_acc: 0.9927\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0292 - val_acc: 0.9938\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 160s 3ms/step - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0293 - val_acc: 0.9928\n",
      "Epoch 54/500\n",
      "20992/60000 [=========>....................] - ETA: 1:34 - loss: 0.0091 - acc: 0.9969"
     ]
    }
   ],
   "source": [
    "# Train the teacher model as usual\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "log_dir = 'logs/teacher/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 256\n",
    "teacher.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6g9inT2hqAU"
   },
   "source": [
    "# Define a new model that outputs only techer logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNuFpi3nh3p6"
   },
   "outputs": [],
   "source": [
    "# Raise the temperature of teacher model and gather the soft targets\n",
    "\n",
    "# Set a tempature value\n",
    "temp = 1\n",
    "\n",
    "#Collect the logits from the previous layer output and store it in a different model\n",
    "teacher_WO_Softmax = Model(teacher.input, teacher.get_layer('dense_2').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "RKe7WUXWgqFy",
    "outputId": "9d462f1e-befb-4f7e-a6bd-56a17313fd19"
   },
   "outputs": [],
   "source": [
    "teacher_WO_Softmax.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oshn9uRTh6j-"
   },
   "source": [
    "# Define a manual softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vb7CuA9h_Mb"
   },
   "outputs": [],
   "source": [
    "# Define a manual softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x)/(np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtROpLm-iBb9"
   },
   "source": [
    "# Understanding the concept of temperature in softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "colab_type": "code",
    "id": "6yzduRJSiAsu",
    "outputId": "b4ae4feb-8b99-4137-8f32-8de7ef942da6"
   },
   "outputs": [],
   "source": [
    "# For example, just grab the first image and lets see how softening of probabilities work\n",
    "intermediate_output = teacher_WO_Softmax.predict(X_test[9].reshape(1,28,28,1))\n",
    "print(softmax(intermediate_output))\n",
    "\n",
    "pixels = X_test[9]\n",
    "pixels = pixels.reshape((28, 28))\n",
    "plt.imshow(pixels)\n",
    "plt.savefig('Kimg.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# logits for the first number in test dataset\n",
    "x = intermediate_output[0]\n",
    "plt.figure(figsize=(20, 10));\n",
    "\n",
    "temperature = [1,3,7,10,20,50]\n",
    "\n",
    "for temp in temperature:\n",
    "    plt.plot((softmax(x/temp)), label='$T='+str(temp)+'$', linewidth=2);\n",
    "    plt.legend();\n",
    "plt.xlabel('classes ->');\n",
    "plt.ylabel('probability');\n",
    "plt.xlim([0, 10]);\n",
    "plt.savefig('Kgraph.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvNRMD9HiMnr"
   },
   "source": [
    "# Prepare the soft targets and the target data for student to be trained upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDei6m9ZiMKU"
   },
   "outputs": [],
   "source": [
    "temp = 1\n",
    "epochs = 500\n",
    "teacher_train_logits = teacher_WO_Softmax.predict(X_train)\n",
    "teacher_test_logits = teacher_WO_Softmax.predict(X_test) # This model directly gives the logits ( see the teacher_WO_softmax model above)\n",
    "\n",
    "# Perform a manual softmax at raised temperature\n",
    "train_logits_T = teacher_train_logits/temp\n",
    "test_logits_T = teacher_test_logits / temp \n",
    "\n",
    "#Y_train_soft = softmax(train_logits_T)\n",
    "#Y_test_soft = softmax(test_logits_T)\n",
    "\n",
    "Y_train_soft = []\n",
    "Y_test_soft = []\n",
    "\n",
    "for i in range( len( train_logits_T ) ):\n",
    "  Y_train_soft.append( softmax( train_logits_T[i] ) )\n",
    "\n",
    "for i in range( len( test_logits_T ) ):\n",
    "  Y_test_soft.append( softmax( test_logits_T[i] ) )\n",
    "\n",
    "#len(Y_train_soft[0])\n",
    "Y_train_soft = np.array(Y_train_soft)\n",
    "Y_test_soft = np.array(Y_test_soft)\n",
    "\n",
    "\n",
    "# Concatenate so that this becomes a 10 + 10 dimensional vector\n",
    "Y_train_new = np.concatenate([Y_train, Y_train_soft], axis=1)\n",
    "Y_test_new =  np.concatenate([Y_test, Y_test_soft], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "FLFpB2hKieMp",
    "outputId": "8a220610-9261-4a5a-951e-97cd97894c11"
   },
   "outputs": [],
   "source": [
    "Y_train_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOLwI1vTh9Id"
   },
   "outputs": [],
   "source": [
    "# This is a standalone student model (same number of layers as original student model) trained on same data\n",
    "# for comparing it with teacher trained student.\n",
    "student = Sequential()\n",
    "student.add(Flatten(input_shape=input_shape))\n",
    "student.add(Dense(32, activation='relu'))\n",
    "student.add(Dropout(0.2))\n",
    "student.add(Dense(nb_classes))\n",
    "student.add(Activation('softmax'))\n",
    "\n",
    "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "student.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy']\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "Df5P49XRjDyQ",
    "outputId": "abce734f-3307-481b-d270-77a77d7e4d48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/pure_student/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "student.fit(X_train, Y_train,\n",
    "          batch_size=256,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),\n",
    "           callbacks=[logging,checkpoint] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y90wE1T_rIRL"
   },
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SoRFn3vGsi8A"
   },
   "outputs": [],
   "source": [
    "studentX = Sequential()\n",
    "studentX.add(Flatten(input_shape=input_shape))\n",
    "studentX.add(Dense(32, activation='relu'))\n",
    "studentX.add(Dropout(0.2))\n",
    "studentX.add(Dense(nb_classes))\n",
    "studentX.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "studentX.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy']\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "x6gRyWwRs4_3",
    "outputId": "49c85bf4-f895-48fd-9b16-36ae30164c98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/no_loss_function/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "\n",
    "studentX.fit(X_train, Y_train_soft,\n",
    "                      batch_size=256,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(X_test, Y_test),\n",
    "                      callbacks=[logging,checkpoint] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcbgRhIIrTNl"
   },
   "source": [
    "# StudentA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "jRPkY70-I434",
    "outputId": "4387db0b-4f69-4178-c350-84a52ccfd0b4"
   },
   "outputs": [],
   "source": [
    "studentA = Sequential()\n",
    "studentA.add(Flatten(input_shape=input_shape))\n",
    "studentA.add(Dense(32, activation='relu'))\n",
    "studentA.add(Dropout(0.2))\n",
    "studentA.add(Dense(nb_classes))\n",
    "studentA.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "##sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "studentA.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#studentA = Model(student.input,student.output)\n",
    "studentA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "B8MbPXV7it21",
    "outputId": "505d7cdb-33ce-4ef4-edd0-0c288602b99f"
   },
   "outputs": [],
   "source": [
    "# Remove the softmax layer from the student network\n",
    "#student.layers.pop()\n",
    "\n",
    "# Now collect the logits from the last layer\n",
    "logits = studentA.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "# softed probabilities at raised temperature\n",
    "logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "\n",
    "# This is our new student model\n",
    "studentA = Model(studentA.input, output)\n",
    "\n",
    "studentA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentA.predict( X_train[0].reshape(1,28,28,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVz7tqSIi4GN"
   },
   "outputs": [],
   "source": [
    "# This will be a teacher trained student model. \n",
    "# --> This uses a knowledge distillation loss function\n",
    "\n",
    "# Declare knowledge distillation loss\n",
    "def knowledge_distillation_loss(y_true, y_pred, alpha):\n",
    "\n",
    "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
    "    y_true, y_logits = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
    "    \n",
    "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
    "    \n",
    "    loss = ( alpha*temp*logloss(y_logits, y_pred) ) + ( (1-alpha)*logloss(y_true,y_pred) ) \n",
    "    \n",
    "    return loss\n",
    "\n",
    "# For testing use regular output probabilities - without temperature\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "  \n",
    "# For testing use regular output probabilities - without temperature\n",
    "def true_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "def logits_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, nb_classes:]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "studentA.compile(\n",
    "    #optimizer=optimizers.SGD(lr=1e-1, momentum=0.9, nesterov=True),\n",
    "    optimizer='adadelta',\n",
    "    loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 0.5),\n",
    "    #loss='categorical_crossentropy',\n",
    "    metrics=[acc] )#,true_loss,logits_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "697eUAMOi_Y0",
    "outputId": "42d53797-0f3e-4be9-f221-03507cbc07d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/loss_function_a/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "studentA.fit(X_train, Y_train_new,\n",
    "                      batch_size=256,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(X_test, Y_test_new),\n",
    "            callbacks=[logging,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DS80ohB0rbz9"
   },
   "source": [
    "# StudentB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "P-wb1CtCruYw",
    "outputId": "31cb533d-e088-48b0-c321-6ed0e38f32f4"
   },
   "outputs": [],
   "source": [
    "studentB = Sequential()\n",
    "studentB.add(Flatten(input_shape=input_shape))\n",
    "studentB.add(Dense(32, activation='relu'))\n",
    "studentB.add(Dropout(0.2))\n",
    "studentB.add(Dense(nb_classes))\n",
    "studentB.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "##sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "studentB.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "#studentB = Model(student.input,student.output)\n",
    "studentB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "SBoelaeMmCim",
    "outputId": "71504111-ec8a-400d-ba09-4a942e56d40a"
   },
   "outputs": [],
   "source": [
    "# Remove the softmax layer from the student network\n",
    "#student.layers.pop()\n",
    "\n",
    "# Now collect the logits from the last layer\n",
    "logits = studentB.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "# softed probabilities at raised temperature\n",
    "#logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits)#(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "\n",
    "# This is our new student model\n",
    "studentB = Model(studentB.input, output)\n",
    "\n",
    "studentB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentB.predict( X_train[0].reshape(1,28,28,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5w93LM2kHh5"
   },
   "outputs": [],
   "source": [
    "# This will be a teacher trained student model. \n",
    "# --> This uses a knowledge distillation loss function\n",
    "\n",
    "# Declare knowledge distillation loss\n",
    "def knowledge_distillation_loss(y_true, y_pred, alpha,beta,gamma):\n",
    "\n",
    "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
    "    y_true, y_logits = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
    "    \n",
    "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
    "    \n",
    "    loss = ( alpha*logloss(y_true,y_logits) ) + ( beta*logloss(y_true, y_pred) ) +( gamma*logloss(y_logits, y_pred) )\n",
    "   \n",
    "    return loss\n",
    "\n",
    "# For testing use regular output probabilities - without temperature\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "  \n",
    "# For testing use regular output probabilities - without temperature\n",
    "def teacher_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_true[:, nb_classes:]\n",
    "    return logloss(y_true, y_pred)\n",
    "  \n",
    "def student_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "def apprentice_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, nb_classes:]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "studentB.compile(\n",
    "    #optimizer=optimizers.SGD(lr=1e-1, momentum=0.9, nesterov=True),\n",
    "    optimizer='adadelta',\n",
    "    loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 1,0.5,0.5),\n",
    "    #loss='categorical_crossentropy',\n",
    "    metrics=[acc] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "NG14ErXnmbh3",
    "outputId": "88d76943-3d05-405f-aa5c-3bf22df5940c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/loss_function_b/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "studentB.fit(X_train, Y_train_new,\n",
    "                      batch_size=256,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(X_test, Y_test_new),\n",
    "            callbacks=[logging,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Knowledge-Distillation_AI.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
